{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Backpropagation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjennings955/Neural-Network-Notebooks/blob/master/Backpropagation_Object_Oriented.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jhW07Y333Wls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# https://pastebin.com/Yc7RBFpT"
      ]
    },
    {
      "metadata": {
        "id": "jpqUgZk-0zFe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer(object):\n",
        "    def __init__(self, parent_layer, *args, **kwargs):\n",
        "        self.parent = parent_layer\n",
        "        self.name = kwargs.pop('name', 'unnamed')\n",
        "        if self.parent:\n",
        "            self.parent.child = self\n",
        "        self.child = None\n",
        "        \n",
        "      \n",
        "# Dense/fully connected layer/Affine layer\n",
        "class Dense(Layer):\n",
        "    def __init__(self, num_inputs, num_outputs, parent_layer=None, name=\"Dense\", *args, **kwargs):\n",
        "        super(Dense, self).__init__(parent_layer)\n",
        "        self.W = np.random.randn(num_outputs, num_inputs)/np.sqrt(num_outputs)\n",
        "        self.b = np.random.randn(num_outputs, 1)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.matmul(self.W, x) + self.b\n",
        "\n",
        "    def backward(self, g):\n",
        "        self.dW = g.dot(self.x.T)\n",
        "        dX = self.W.T.dot(g)\n",
        "        self.db = g\n",
        "        return dX\n",
        "    def __repr__(self):\n",
        "        return \"Dense(num_inputs={}, num_outputs={}, name={})\".format(self.W.shape[0], self.W.shape[0], self.name)\n",
        "       \n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self, parent_layer, *args, **kwargs):\n",
        "        super(Sigmoid, self).__init__(parent_layer, *args, **kwargs)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.out = 1/(1 + np.exp(-x))\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, g):\n",
        "        sigmoid_gradient = self.out*(1 - self.out)\n",
        "        return sigmoid_gradient*g\n",
        "    def __repr__(self):\n",
        "        return \"Sigmoid(name={})\".format(self.name)\n",
        "      \n",
        "class Softmax(Layer):\n",
        "    def __init__(self, parent_layer, *args, **kwargs):\n",
        "        super(Softmax, self).__init__(parent_layer, *args, **kwargs)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        e_x = np.exp(x)\n",
        "        denominator = np.sum(e_x)\n",
        "        self.output = e_x/denominator\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, g):\n",
        "        jacobian = np.diagflat(self.output) - np.matmul(self.output, self.output.T)\n",
        "        return np.matmul(jacobian.T, g)\n",
        "      \n",
        "    def __repr__(self):\n",
        "        return \"Softmax(name={})\".format(self.name)\n",
        "      \n",
        "class CrossEntropy(Layer):\n",
        "    def __init__(self, parent_layer, labels, *args, **kwargs):\n",
        "        super(CrossEntropy, self).__init__(parent_layer)\n",
        "        self.labels = labels # We really should have figured out a better way to implement this.\n",
        "        \n",
        "    def forward(self, y_hat):\n",
        "        self.y_hat = y_hat\n",
        "        self.y = self.labels\n",
        "        xent = -np.sum(self.y * np.log(self.y_hat + 1e-8))\n",
        "        return xent\n",
        "        \n",
        "    def backward(self, g):\n",
        "        self.gradient = self.y / self.y_hat\n",
        "        return self.gradient\n",
        "      \n",
        "    def __repr__(self):\n",
        "        return \"CrossEntropy(name={})\".format(self.name) \n",
        "\n",
        "class Network(Layer):\n",
        "    def __init__(self, parent_layer):\n",
        "        super(Network, self).__init__(parent_layer)\n",
        "\n",
        "    def _root(self):\n",
        "        obj = self.parent\n",
        "        while obj.parent != None:\n",
        "            obj = obj.parent\n",
        "            self.root = obj\n",
        "        return self.root\n",
        "\n",
        "    def forward(self, input):\n",
        "        obj = self._root()\n",
        "        current_input = input\n",
        "        while obj.child:\n",
        "            out = obj.forward(current_input)\n",
        "            current_input = out\n",
        "            print('call', obj, '.forward(X) with output from previous layer,\\noutput={}'.format(out))\n",
        "            print('-----------------------------')\n",
        "            obj = obj.child\n",
        "\n",
        "    def backward(self):\n",
        "        g = 1\n",
        "        obj = self.parent\n",
        "    \n",
        "        while obj:\n",
        "            g = obj.backward(g)\n",
        "            print('call', obj, '.backward(g) with g flowing backwards from child layer\\ng={}'.format(g))\n",
        "            print('-----------------------------')\n",
        "            obj = obj.parent\n",
        "    \n",
        "    def __repr__(self):\n",
        "        output = []\n",
        "        obj = self._root()\n",
        "        while obj.child:\n",
        "          output.append(repr(obj))\n",
        "          obj = obj.child\n",
        "        return ' -> '.join(output)\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "# z_1 = Sigmoid(net_1)\n",
        "# net_2 = Dense(3,3, z_1)\n",
        "# z_2 = Softmax(net_2)\n",
        "# loss = CrossEntropy(z_2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_4a_0QMkRrHk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing Dense layer"
      ]
    },
    {
      "metadata": {
        "id": "0lUop46C5SqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(1234)\n",
        "net_1 = Dense(num_inputs=2, num_outputs=3)\n",
        "print(net_1.W)\n",
        "print(net_1.b)\n",
        "\n",
        "# 0.2, -0.3\n",
        "\n",
        "x = np.float32([[0.2, -0.3]]).T\n",
        "g = np.float32([[1, 1, 1]]).T\n",
        "print(net_1.forward(x))\n",
        "print(net_1.backward(g))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KRXzn-HAWW22",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-BYo2QCRujx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing Sigmoid layer"
      ]
    },
    {
      "metadata": {
        "id": "tKFb2v825TLT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = np.float32([[0.0, -5.0, 5.0]]).T\n",
        "s = Sigmoid(parent_layer=None)\n",
        "g = np.float32([[1, 1, 1]]).T\n",
        "print(s.forward(x))\n",
        "print(s.backward(g))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_bF8G5jdRwwH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing Softmax Layer"
      ]
    },
    {
      "metadata": {
        "id": "r69e1kKN85UN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = np.float32([[1, 1, 1]]).T\n",
        "g = np.float32([[-1, 1, 0]]).T\n",
        "soft = Softmax(None)\n",
        "print(soft.forward(x))\n",
        "print(soft.backward(g))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbdifv8eRzZk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing Cross Entropy layer "
      ]
    },
    {
      "metadata": {
        "id": "Yt69GoS5_TEV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = np.float32([[1, 0, 0]]).T\n",
        "x_ent = CrossEntropy(None, labels=y)\n",
        "y_hat = np.float32([[0.25, 0.4, 0.35]]).T\n",
        "print(x_ent.forward(y_hat))\n",
        "print(x_ent.backward(1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bP6I9zrVR4Oa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating a network and connecting it all-together using the Network class"
      ]
    },
    {
      "metadata": {
        "id": "QNE8LpnPEvYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = np.float32([[1, 0, 0]]).T\n",
        "\n",
        "net_1 = Dense(2, 10, name='dense_1')\n",
        "z_1 = Sigmoid(net_1, name='sigmoid_1')\n",
        "net_2 = Dense(10,3, z_1, name='dense_2')\n",
        "z_2 = Softmax(net_2, name='softmax_outut')\n",
        "loss = CrossEntropy(z_2, labels=y, name='xent')\n",
        "network = Network(loss)\n",
        "print(network)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UcI7w9CoSA9f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Forward Propagation"
      ]
    },
    {
      "metadata": {
        "id": "__GmrWJNFy5p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = np.float32([[0.2, -0.3]]).T\n",
        "network.forward(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MIqAPMdOSDaU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Compute the backward pass for a single input"
      ]
    },
    {
      "metadata": {
        "id": "MtvNgt26F-q6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "network.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mp5RWco8SLpA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inspecting the gradient of our loss wrt our weights and biases"
      ]
    },
    {
      "metadata": {
        "id": "qqK6vnJzJNUq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net_1.dW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ErmU3JShJ0KH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net_1.db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwqC2CRTSZpO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(net_2.W)\n",
        "print(net_2.b)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}